{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've now used a network with cifar10 and played with the number of epochs to show that it matters how many times you train the network on the same data.\n",
    "\n",
    "MNIST can also be called into pytorch the same way that CIFAR10 was, but I intentionally avoided doing that.\n",
    "\n",
    "The point of modifying this code further to use the MNIST dataset downloaded from the internet was to show that I can use different datasets and know how to load them myself without relying on datasets provided by pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's first import the tools (modules) from pytorch we'll need to get everything done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "We transform them to Tensors of normalized range [-1, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "transforms.Pad(2),\n",
    "transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# MNIST is already 28x28 pixels uniformly across the dataset, so we don't need to transform \n",
    "# the data to make it uniform\n",
    "\n",
    "# We do need to resize the images to 32x32 from 28x28 to reuse the network architecture from cifar10, however\n",
    "# In this case, I've just added a 2px pad around each border of the images to make them 32x32\n",
    "\n",
    "\n",
    "# The ImageFolder function assumes the data is structured such that the folder containing \n",
    "# the images is the class for that image, which is how the MNIST dataset is structured\n",
    "trainset = datasets.ImageFolder(root='/Users/etheredgej/Desktop/MNIST/training/',\n",
    "                                           transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.ImageFolder(root='/Users/etheredgej/Desktop/MNIST/testing/',\n",
    "                                           transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "# data loader is an alternative to PIL (python imaging library) that loads the images into a numpy array\n",
    "\n",
    "classes = ('0','1','2','3','4','5','6','7','8','9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transformed the images even though they're of uniform size. \n",
    "MNIST is already 28x28 pixels uniformly across the dataset, so we don't need to transform the data to make it uniform.\n",
    "\n",
    "We do need to resize the images to 32x32 from 28x28 to reuse the network architecture from cifar10, however.\n",
    "In this case, I've just added a 2px pad around each border of the images to make them 32x32.\n",
    "\n",
    "In the future, I'll use an appropriately sized network, since using a larger than necessary number of nodes in our input layer is a waste. \n",
    "\n",
    "The pixels are just ignored after training, as we would hope, but it's not an elegant solution.\n",
    "\n",
    "The ImageFolder function assumes the data is structured such that the folder containing the images is the class for that image, which is how the MNIST dataset is structured.\n",
    "\n",
    "The torchvision data loader is an alternative to PIL (python imaging library). Using PIL, we would loads the images into a numpy array of pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "# why did I add these again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show some of the normalized training images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth:      1     1     4     4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c32b630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader) # define dataiter to pull next batch of images from training dataset\n",
    "images, labels = dataiter.next() # get the next set of 4 images with corresponding labels\n",
    "\n",
    "# show normalized images\n",
    "img=torchvision.utils.make_grid(images)\n",
    "plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "# print labels\n",
    "print('Ground truth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to a sample of the unnormalized images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth:      2     4     4     5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFegiIjmhAl1EJCdUoIuI5MT/A/P5lOp4AVyhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d324080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = dataiter.next() # get the next set of 4 images with corresponding labels\n",
    "\n",
    "# show unnormalized images\n",
    "img=torchvision.utils.make_grid(images)\n",
    "img = img / 2 + 0.5     # unnormalize the images\n",
    "plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "# print labels\n",
    "print('Ground truth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm reusing the 32x32 3-channel network from CIFAR10. In the future I'll use an appropriately sized network (1 channel 28x28), but we'll see that this still works quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a Loss function and optimizer\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "Let's use a Classification Cross-Entropy loss and SGD with momentum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lr and momentum variables are where the interesting things happen.\n",
    "\n",
    "All learning algorithms are solving gradient descent, and a learning rate that's too high or too low are both bad.\n",
    "\n",
    "The learning rate weights how large a step to take during gradient descent.\n",
    "\n",
    "Momentum is how much we're incorporating the previous direction, which is important because we can only load a subset of the data into memory.\n",
    "\n",
    "Adam is a better optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the network.\n",
    "\n",
    "This is when things start to get interesting.\n",
    "\n",
    "Next we will loop over our data iterator, and feed the inputs to the network and optimize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.231\n",
      "[1,  4000] loss: 0.221\n",
      "[1,  6000] loss: 0.154\n",
      "[1,  8000] loss: 0.124\n",
      "[1, 10000] loss: 0.103\n",
      "[1, 12000] loss: 0.100\n",
      "[1, 14000] loss: 0.089\n",
      "Accuracy of the network on the 10000 test images: 97 %\n",
      "Accuracy of     0 : 98 %\n",
      "Accuracy of     1 : 98 %\n",
      "Accuracy of     2 : 97 %\n",
      "Accuracy of     3 : 98 %\n",
      "Accuracy of     4 : 97 %\n",
      "Accuracy of     5 : 99 %\n",
      "Accuracy of     6 : 96 %\n",
      "Accuracy of     7 : 99 %\n",
      "Accuracy of     8 : 89 %\n",
      "Accuracy of     9 : 96 %\n",
      "[2,  2000] loss: 0.070\n",
      "[2,  4000] loss: 0.072\n",
      "[2,  6000] loss: 0.072\n",
      "[2,  8000] loss: 0.070\n",
      "[2, 10000] loss: 0.062\n",
      "[2, 12000] loss: 0.061\n",
      "[2, 14000] loss: 0.057\n",
      "Accuracy of the network on the 10000 test images: 98 %\n",
      "Accuracy of     0 : 99 %\n",
      "Accuracy of     1 : 97 %\n",
      "Accuracy of     2 : 99 %\n",
      "Accuracy of     3 : 99 %\n",
      "Accuracy of     4 : 98 %\n",
      "Accuracy of     5 : 98 %\n",
      "Accuracy of     6 : 97 %\n",
      "Accuracy of     7 : 98 %\n",
      "Accuracy of     8 : 99 %\n",
      "Accuracy of     9 : 98 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(Variable(images))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(Variable(images))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i]\n",
    "            class_total[label] += 1\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    \n",
    "print('Finished Training')\n",
    "\n",
    "# When does the algorithm stop learning and start over-fitting?\n",
    "# We want to determine this trend and then cut off our training before over-fitting starts.\n",
    "# I updated the number of epochs from 5 to 30 (5 epochs means it sees each image 5 times)\n",
    "# Tensorboard version for pytorch to show the loss over time\n",
    "# or just graph the loss numbers over time to observe the trends\n",
    "\n",
    "# Learning rate = How fast to perform gradient descent\n",
    "\n",
    "# Check whether cats are under-represented in the training dataset, since it performed by far worst on cats.\n",
    "\n",
    "# Put in an SVM classifier into this. - I'll need to find out how to load all the data at once rather than use the dataloader from this set.\n",
    "\n",
    "# Try using 3 classifiers instead of 10.\n",
    "# Computers, trees, something else - 10 images of each\n",
    "# Test on the training dataset - it will perform perfectly\n",
    "# Alternatively, find another dataset to use, so long as I'm loading it myself rather than using the dataloader\n",
    "\n",
    "# Try on non-image data as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've printed the averaged accuracy along with the accuracy for each digit after each epoch.\n",
    "\n",
    "We can see that this is a much easier problem to solve than the CIFAR10 dataset. The convolution network gets 97% accuracy after a single round of training and 98% accuracy after 2 rounds. In a previous run, I got 98% accuracy after a single round of training, so we can see that this is pretty consistent between training sessions.\n",
    "\n",
    "We can also see that it performs quite well on every digit. There aren't examples of poorly classified digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've already added accuracy checks at each epoch, but now that we've trained the network, let's test it again to see how it performs re-using the more fun visual representation from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth:      1     4     7     7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6CIiGfH/iXuwsHvoYAwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cf7c9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = dataiter.next() # get the next set of 4 images with corresponding labels\n",
    "\n",
    "# show normalized images\n",
    "img=torchvision.utils.make_grid(images)\n",
    "plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "# print labels\n",
    "print('Ground truth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      " -0.8202   7.2773   1.7581  -2.1656  -1.3041  -3.3534  -1.8478  -1.8616\n",
      " -4.5476   0.7416  -1.2497  -3.6737  11.4589  -3.2505  -2.5717   0.9189\n",
      " -1.8207   1.4430  -0.6530  -0.0100  -2.3903  -2.5991  -6.5629   9.1279\n",
      " -4.6457   3.0505   1.2968   4.0214  -4.5873  -2.5764  -9.6612  11.8386\n",
      "\n",
      "Columns 8 to 9 \n",
      "  4.1157  -1.1103\n",
      " -1.4588   2.8885\n",
      "  0.3829   1.0914\n",
      " -1.2266   1.4965\n",
      "[torch.FloatTensor of size 4x10]\n",
      "\n",
      "('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n"
     ]
    }
   ],
   "source": [
    "outputs = net(Variable(images))\n",
    "print(outputs)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are \"energies\" for the 10 classes.\n",
    "\n",
    "By printing both the outputs and the classes, we can see that the network has a prediction for how likely it thinks each of the 4 images is to fall into each of the 10 classes. The higher the energy for a class, the more the network\n",
    "thinks that the image is of the particular class. For instance, if we look at row 1 column 2, we can see that this is a much higher value for that row than in the other columns. Column 2 corresponds to digit '1', which is what the network has predicted for the first image.\n",
    "\n",
    "The class with the highest energy is the one that the network is predicting. So to get the predicted class, let's get the index of the highest energy and show that prediction along with the grount truth class and images from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth:      1     4     7     7\n",
      "Predicted:      1     4     7     7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6CIiGfH/iXuwsHvoYAwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c67b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show normalized images\n",
    "img=torchvision.utils.make_grid(images)\n",
    "plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "# print labels\n",
    "print('Ground truth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem pretty good, at least for these 4 images, showing the ground truth and prediction side-by-side.\n",
    "\n",
    "Let us look at how the network performs on the whole dataset again (which we checked after each epoch, so it should perform as well as it did after the last epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network clearly learned, with it ultimately performing at 98% accuracy averaged over all 10 classes.\n",
    "\n",
    "It is performing far better than chance, which is 10%. This is the measure of the accuracy across all classes, however.\n",
    "\n",
    "Next, let's look at the performance of predicting each class, as it is expected that not all classes trained equally well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of     0 : 99 %\n",
      "Accuracy of     1 : 97 %\n",
      "Accuracy of     2 : 99 %\n",
      "Accuracy of     3 : 99 %\n",
      "Accuracy of     4 : 98 %\n",
      "Accuracy of     5 : 98 %\n",
      "Accuracy of     6 : 97 %\n",
      "Accuracy of     7 : 98 %\n",
      "Accuracy of     8 : 99 %\n",
      "Accuracy of     9 : 98 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98% accuracy average and 97-99% within each class. Amazing for only 2 training sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've tried a neural network both with and without convolution layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
